# =============================================================================
# Personal Knowledge Engine - Environment Variables
# =============================================================================
# Copy this file to .env and fill in your actual values:
#   cp .env.example .env
#
# IMPORTANT: Never commit .env to git (it contains secrets)
# =============================================================================

# =============================================================================
# PROVIDER SELECTION
# =============================================================================
# Choose which provider to use for embeddings and LLM

# Embedding Provider: "local" or "openai"
# - local: Uses sentence-transformers (free, private, works offline)
# - openai: Uses OpenAI Ada-002 (better quality, requires API key)
EMBEDDING_PROVIDER=local

# LLM Provider: "local" or "openai"
# - local: Uses llama.cpp with local GGUF model (free, private)
# - openai: Uses GPT-4 (best quality, requires API key)
LLM_PROVIDER=local

# =============================================================================
# OPENAI CONFIGURATION
# =============================================================================
# Required if using openai as embedding or LLM provider
# Get your key at: https://platform.openai.com/api-keys

OPENAI_API_KEY=sk-your-openai-api-key-here

# Optional: Override default models
# OPENAI_EMBEDDING_MODEL=text-embedding-ada-002
# OPENAI_CHAT_MODEL=gpt-4-turbo-preview

# =============================================================================
# GOOGLE API CONFIGURATION (for Gmail/Drive integration)
# =============================================================================
# Set up at: https://console.cloud.google.com/
# 1. Create a project
# 2. Enable Gmail API and Google Drive API
# 3. Create OAuth 2.0 credentials (Web application)
# 4. Add redirect URI: http://localhost:8000/api/auth/google/callback

GOOGLE_CLIENT_ID=your-client-id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your-client-secret

# =============================================================================
# LOCAL LLM CONFIGURATION (llama.cpp)
# =============================================================================
# Required if using local LLM provider
# Download a GGUF model from HuggingFace:
# https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF

# Path to your GGUF model file
LLM_MODEL_PATH=./data/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# GPU acceleration (set > 0 if you have a compatible GPU)
LLM_GPU_LAYERS=0

# Model parameters
LLM_CONTEXT_LENGTH=4096
LLM_MAX_TOKENS=1024
LLM_TEMPERATURE=0.7

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================

# Debug mode (enables detailed logging)
DEBUG=true

# Chunking settings
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# Search settings
SEARCH_TOP_K=5
HYBRID_SEARCH_SEMANTIC_WEIGHT=0.7

# Server settings
HOST=0.0.0.0
PORT=8000
