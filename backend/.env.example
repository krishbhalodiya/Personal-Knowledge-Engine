# =============================================================================
# Personal Knowledge Engine - Environment Variables
# =============================================================================
# Copy this file to .env and fill in your actual values:
#   cp .env.example .env
#
# IMPORTANT: Never commit .env to git (it contains secrets)
# =============================================================================

# =============================================================================
# PROVIDER SELECTION
# =============================================================================
# Choose which provider to use for embeddings and LLM

# Embedding Provider: "local" or "openai"
# - local: Uses sentence-transformers (free, private, works offline)
# - openai: Uses OpenAI embeddings (better quality, requires API key)
EMBEDDING_PROVIDER=openai

# LLM Provider: "local", "openai", or "gemini"
# - local: Uses llama.cpp with local GGUF model (free, private)
# - openai: Uses GPT-4 (best quality, requires API key)
# - gemini: Uses Google Gemini (great quality, free tier available)
LLM_PROVIDER=openai

# =============================================================================
# OPENAI CONFIGURATION
# =============================================================================
# Required if using openai as embedding or LLM provider
# Get your key at: https://platform.openai.com/api-keys

OPENAI_API_KEY=sk-your-openai-api-key-here

# OpenAI Embedding Model Options:
# - text-embedding-ada-002: 1536 dims, older but reliable ($0.0001/1K tokens)
# - text-embedding-3-small: 1536 dims, better quality, cheaper ($0.02/1M tokens)
# - text-embedding-3-large: 3072 dims, best quality, more expensive ($0.13/1M tokens)
#
# Recommendation: Use text-embedding-3-small for most cases (best quality/price)
#                  Use text-embedding-3-large for critical applications
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# OpenAI Chat Model Options:
# - gpt-4o: Latest GPT-4 Optimized (best quality, fast)
# - gpt-4-turbo-preview: GPT-4 Turbo (great quality)
# - gpt-3.5-turbo: Faster, cheaper (good for simple tasks)
OPENAI_CHAT_MODEL=gpt-4o

# =============================================================================
# GOOGLE GEMINI CONFIGURATION
# =============================================================================
# Required if using gemini as LLM provider
# Get your key at: https://aistudio.google.com/app/apikey
# Free tier: 60 requests/minute, 1M tokens/day

GOOGLE_GEMINI_API_KEY=your-gemini-api-key-here

# Gemini Model Options:
# - gemini-2.0-flash-exp: Latest experimental, fastest, best for most tasks
# - gemini-1.5-pro: Best quality, longer context (1M tokens!)
# - gemini-1.5-flash: Fast and efficient, good balance
GOOGLE_GEMINI_MODEL=gemini-2.0-flash-exp

# =============================================================================
# GOOGLE API CONFIGURATION (for Gmail/Drive integration)
# =============================================================================
# Set up at: https://console.cloud.google.com/
# 1. Create a project
# 2. Enable Gmail API and Google Drive API
# 3. Create OAuth 2.0 credentials (Web application)
# 4. Add redirect URI: http://localhost:8000/api/auth/google/callback

GOOGLE_CLIENT_ID=your-client-id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your-client-secret

# Optional: Change callback URL for production
# GOOGLE_REDIRECT_URI=http://localhost:8000/api/auth/google/callback

# =============================================================================
# LOCAL LLM CONFIGURATION (llama.cpp)
# =============================================================================
# Required if using local LLM provider
# Download a GGUF model from HuggingFace:
# https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF

# Path to your GGUF model file
LLM_MODEL_PATH=./data/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# GPU acceleration (set > 0 if you have a compatible GPU)
LLM_GPU_LAYERS=0

# Model parameters
LLM_CONTEXT_LENGTH=4096
LLM_MAX_TOKENS=1024
LLM_TEMPERATURE=0.7

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================

# Debug mode (enables detailed logging)
DEBUG=true

# Chunking settings
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# Search settings
SEARCH_TOP_K=5
HYBRID_SEARCH_SEMANTIC_WEIGHT=0.7

# Server settings
HOST=0.0.0.0
PORT=8000
